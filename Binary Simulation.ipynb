{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "153657fd-ad54-452e-b296-f65b6a0628b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from econml.dml import LinearDML\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output (e.g., convergence warnings in high-dim logistic reg)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953c82ab-3d5b-4313-9def-e9d6634475b9",
   "metadata": {},
   "source": [
    "## Non linear simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e53b3033-8443-4f08-8d17-fc4330c92e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bin_data(n=500, p=100, seed=123):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # 1. Generate Covariates (100 variables)\n",
    "    X = np.random.normal(0, 1, size=(n, p))\n",
    "    \n",
    "    # 2. Define the &quot;True&quot; Confounding Mechanism (The Nuisance Function)\n",
    "    # Only the first 5 variables (indices 0-4) actually matter.\n",
    "    # Interaction: X0 * X1\n",
    "    # Non-linear: X2 squared\n",
    "    # Linear: X3, X4\n",
    "    nuisance_term = 0.5 * X[:,0] * X[:,1] + 0.4 * (X[:,2]**2) + 0.3 * X[:,3] + 0.2 * X[:,4]\n",
    "    # 3. Treatment Assignment (Propensity)\n",
    "    # P(T=1 | X) depends on the nuisance term\n",
    "    logit_p = -0.5 + nuisance_term\n",
    "    prob_t = 1 / (1 + np.exp(-logit_p))\n",
    "    T = np.random.binomial(1, prob_t)\n",
    "    # 4. Outcome Generate\n",
    "    # True Causal Effect is ~14.2% Risk Difference (calculated via simulation below)\n",
    "    \n",
    "    # Base Log-Odds (without treatment)\n",
    "    logit_y_base = -1.5 + nuisance_term\n",
    "    \n",
    "    # To get the Ground Truth Risk Difference, we simulate the counterfactuals:\n",
    "    # What if everyone was treated?\n",
    "    p1_true = 1 / (1 + np.exp(-(logit_y_base + 0.8 * 1))) \n",
    "    # What if everyone was untreated?\n",
    "    p0_true = 1 / (1 + np.exp(-(logit_y_base + 0.8 * 0)))\n",
    "    \n",
    "    true_ate = np.mean(p1_true - p0_true)\n",
    "    \n",
    "    # Generate Observed Binary Outcome\n",
    "    # We add the treatment effect (0.8 log odds) to the base\n",
    "    logit_y_observed = logit_y_base + 0.8 * T\n",
    "    prob_y_observed = 1 / (1 + np.exp(-logit_y_observed))\n",
    "    Y = np.random.binomial(1, prob_y_observed)\n",
    "    Y = Y.astype(int)\n",
    "    \n",
    "    return X, T, Y, true_ate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d49abdb-795f-4e4a-8631-99c3d7ee8550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalEstimators:\n",
    "\n",
    "# A helper class to run various causal inference methods:\n",
    "# 1. Naive (Unadjusted Difference in Means)\n",
    "# 2. Regression Adjustment (Linear/Logistic)\n",
    "# 3. IPTW (Inverse Probability of Treatment Weighting)\n",
    "# 4. PSM (Propensity Score Matching 1:1)\n",
    "# 5. DML (Double Machine Learning)\n",
    "\n",
    "    def __init__(self, X, T, Y, outcome_type= 'continuous'):\n",
    "        self.X = X\n",
    "        self.T = T\n",
    "        self.Y = Y\n",
    "        self.outcome_type = outcome_type\n",
    "        # Create a DataFrame for easier handling in naive methods\n",
    "        self.df = pd.DataFrame(X)\n",
    "        self.df['T'] = T\n",
    "        self.df['Y'] = Y\n",
    "    def run_naive(self):\n",
    "    \n",
    "        treated = self.df[self.df['T'] == 1]['Y'].mean()\n",
    "        control = self.df[self.df['T'] == 0]['Y'].mean()\n",
    "        return treated - control\n",
    "\n",
    "    def run_regression_adjustment(self):\n",
    "        features = np.column_stack((self.T, self.X))\n",
    "        # Logistic Regression for binary outcome\n",
    "        # We use L2 penalty (default) to help convergence in high dimensions (N=500, P=100)\n",
    "        model = LogisticRegression(solver='liblinear', max_iter=2000) \n",
    "        model.fit(features, self.Y)\n",
    "        \n",
    "        # --- Marginal Effects (Risk Difference) ---\n",
    "        # 1. Create a dataset where EVERYONE is treated (T=1)\n",
    "        X_treated = np.column_stack((np.ones(len(self.X)), self.X))\n",
    "        # 2. Create a dataset where EVERYONE is control (T=0)\n",
    "        X_control = np.column_stack((np.zeros(len(self.X)), self.X))\n",
    "        \n",
    "        # 3. Predict probabilities for both counterfactuals\n",
    "        p1 = model.predict_proba(X_treated)[:, 1]\n",
    "        p0 = model.predict_proba(X_control)[:, 1]\n",
    "        \n",
    "        # 4. Average difference is the Average Treatment Effect (Risk Difference)\n",
    "        return np.mean(p1 - p0)\n",
    "\n",
    "\n",
    "    def run_iptw(self):\n",
    "        # Estimate Propensity Scores\n",
    "        ps_model = LogisticRegression(solver='liblinear', max_iter=2000)\n",
    "        ps_model.fit(self.X, self.T)\n",
    "        ps = ps_model.predict_proba(self.X)[:, 1]\n",
    "        \n",
    "        # Clip to prevent division by zero or extreme weights (common in small N)\n",
    "        ps = np.clip(ps, 0.05, 0.95)\n",
    "        # Calculate weights: 1/PS for treated, 1/(1-PS) for control\n",
    "        weights = np.where(self.T == 1, 1/ps, 1/(1-ps))\n",
    "        # Weighted Difference in Means (Risk Difference)\n",
    "        weighted_mean_1 = np.average(self.Y[self.T==1], weights=weights[self.T==1])\n",
    "        weighted_mean_0 = np.average(self.Y[self.T==0], weights=weights[self.T==0])\n",
    "        return weighted_mean_1 - weighted_mean_0\n",
    "\n",
    "\n",
    "    def run_psm(self):\n",
    "        # Estimate Propensity Scores\n",
    "        ps_model = LogisticRegression(solver='liblinear', max_iter=2000)\n",
    "        ps_model.fit(self.X, self.T)\n",
    "        ps = ps_model.predict_proba(self.X)[:, 1]\n",
    "        \n",
    "        treated_idx = np.where(self.T == 1)[0]\n",
    "        control_idx = np.where(self.T == 0)[0]\n",
    "        \n",
    "        # Safety check for separation\n",
    "        if len(control_idx) == 0 or len(treated_idx) == 0:\n",
    "            return np.nan\n",
    "        # Match each treated unit to nearest control unit based on PS\n",
    "        nbrs = NearestNeighbors(n_neighbors=1).fit(ps[control_idx].reshape(-1, 1))\n",
    "        distances, indices = nbrs.kneighbors(ps[treated_idx].reshape(-1, 1))\n",
    "        matched_control_idx = control_idx[indices.flatten()]\n",
    "        \n",
    "        # Difference in Means of matched pairs (Risk Difference for binary)\n",
    "        return np.mean(self.Y[treated_idx]) - np.mean(self.Y[matched_control_idx])\n",
    "    \n",
    "    def run_dml(self):\n",
    "        # Use shallow trees (depth 2) to prevent overfitting on small N=500\n",
    "        y_model = GradientBoostingClassifier(n_estimators=50, max_depth=2, random_state=42)\n",
    "        t_model = GradientBoostingClassifier(n_estimators=50, max_depth=2, random_state=42)\n",
    "    \n",
    "        # Set cv=3 for small sample size (5-fold might be too thin)\n",
    "        est = LinearDML(model_y=y_model,\n",
    "                        model_t=t_model,\n",
    "                        discrete_outcome = True,\n",
    "                        discrete_treatment=True,\n",
    "                        cv=3,\n",
    "                        random_state=42)\n",
    "        est.fit(self.Y, self.T, X=self.X)\n",
    "        # Return Average Treatment Effect\n",
    "        return est.effect(self.X).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bdfae86-c67a-47db-8be9-5033af64393f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario A: Continuous Outcome (e.g., Total Pay)\n",
      "True Causal Effect: 0.16\n",
      "-----------------------------------------------------------------\n",
      "Method                         | Estimate        | Bias      \n",
      "-----------------------------------------------------------------\n",
      "Naive (Unadjusted)             | 0.20            | 0.04      \n",
      "Regression (Logistic):         | 0.19            | 0.03      \n",
      "IPTW (Logistic PS)             | 0.25            | 0.10      \n",
      "PSM (1:1 Nearest Neighbor)     | 0.22            | 0.06      \n",
      "DML (Gradient Boosting)        | 0.19            | 0.03      \n",
      "\n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Test run to see if the code is working ---\n",
    "X, T, Y, true_eff = generate_bin_data(n=500, p=100,seed = 1)\n",
    "sim = CausalEstimators(X, T, Y, outcome_type='binary')\n",
    "\n",
    "print(f\"Scenario A: Continuous Outcome (e.g., Total Pay)\")\n",
    "print(f\"True Causal Effect: {true_eff:.2f}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Method':<30} | {'Estimate':<15} | {'Bias':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "results_continuous = {\n",
    "\"Naive (Unadjusted)\": sim.run_naive(),\n",
    "\"Regression (Logistic):\": sim.run_regression_adjustment(),\n",
    "\"IPTW (Logistic PS)\": sim.run_iptw(),\n",
    "\"PSM (1:1 Nearest Neighbor)\": sim.run_psm(),\n",
    "\"DML (Gradient Boosting)\": sim.run_dml()\n",
    "}\n",
    "\n",
    "for method, val in results_continuous.items():\n",
    "    bias = val - true_eff\n",
    "    print(f\"{method:<30} | {val:<15.2f} | {bias:<10.2f}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*65 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0465977-53bd-489b-b744-fd9e528d21ad",
   "metadata": {},
   "source": [
    "## Baseline group with 10 covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8095ac33-44f3-4db4-bcfc-37b4ae9f0abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline_simulation(n_epochs=100):\n",
    "    # 1. Initialize a list to store results\n",
    "    results_list = []\n",
    "\n",
    "    print(f\"Starting simulation with {n_epochs} epochs...\")\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        # --- 1. CONTINUOUS OUTCOME ---\n",
    "        X, T, Y, true_eff = generate_bin_data(n=500, p=10, seed = epoch)\n",
    "        sim = CausalEstimators(X, T, Y, outcome_type='continuous')\n",
    "                \n",
    "        # 3. Apply your 5 different methods\n",
    "        # Replace these with your actual function calls (e.g., DML, PSM, etc.)\n",
    "        \n",
    "        # Method 1: Naive (Unadjusted)\n",
    "        res_m1 = sim.run_naive()\n",
    "        \n",
    "        # Method 2: Regression (Linear)\n",
    "        res_m2 = sim.run_regression_adjustment()\n",
    "        \n",
    "        # Method 3: IPTW (Logistic PS)\n",
    "        res_m3 = sim.run_iptw()\n",
    "        \n",
    "        # Method 4: PSM (1:1 Nearest Neighbor)\n",
    "        res_m4 = sim.run_psm()\n",
    "        \n",
    "        # Method 5: DML (Gradient Boosting)\n",
    "        res_m5 = sim.run_dml()\n",
    "\n",
    "        # 4. Record results for each method in a dictionary\n",
    "        # We store them as separate rows to make it \"tidy\" (Long Format)\n",
    "        methods = {\n",
    "            \"Naive (Unadjusted)\": res_m1,\n",
    "            \"Regression (Logistic):\": res_m2,\n",
    "            \"IPTW (Logistic PS)\": res_m3,\n",
    "            \"PSM (1:1 Nearest Neighbor)\": res_m4,\n",
    "            \"DML (Gradient Boosting)\": res_m5\n",
    "        }\n",
    "\n",
    "        for method_name, value in methods.items():\n",
    "            results_list.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"method\": method_name,\n",
    "                \"estimate\": value,\n",
    "                \"bias\": abs(value - true_eff), # You can add metrics here\n",
    "            })\n",
    "\n",
    "    # 5. Convert the list of dictionaries to a pandas DataFrame\n",
    "    df_results = pd.DataFrame(results_list)\n",
    "    \n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e986b11-7f93-4371-8fe2-439322d69415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simulation with 1000 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:49<00:00,  3.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# Execute\n",
    "df = run_baseline_simulation(n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bcffc0f-31f1-4a91-b134-42e7158605e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulation Estimates Summary:\n",
      "                       method      mean       std\n",
      "0     DML (Gradient Boosting)  0.204469  0.045362\n",
      "1          IPTW (Logistic PS)  0.252240  0.043199\n",
      "2          Naive (Unadjusted)  0.271544  0.041403\n",
      "3  PSM (1:1 Nearest Neighbor)  0.253618  0.056115\n",
      "4      Regression (Logistic):  0.237319  0.041023\n"
     ]
    }
   ],
   "source": [
    "# Quick summary of mean estimates across methods\n",
    "summary = df.groupby('method')['estimate'].agg(['mean', 'std']).reset_index()\n",
    "print(\"\\nSimulation Estimates Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1145733-9737-414a-93cc-c8b3b69a85b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulation Bias Summary:\n",
      "                       method      mean       std\n",
      "0     DML (Gradient Boosting)  0.054327  0.036002\n",
      "1          IPTW (Logistic PS)  0.095070  0.042135\n",
      "2          Naive (Unadjusted)  0.113943  0.041329\n",
      "3  PSM (1:1 Nearest Neighbor)  0.098309  0.051947\n",
      "4      Regression (Logistic):  0.080724  0.038914\n"
     ]
    }
   ],
   "source": [
    "# Quick summary of mean estimates across methods\n",
    "summary = df.groupby('method')['bias'].agg(['mean', 'std']).reset_index()\n",
    "print(\"\\nSimulation Bias Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceed56ae-5acc-415e-ac63-fef32d87966e",
   "metadata": {},
   "source": [
    "## Linear data generation with rare disease settings as the control group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "137e7f02-e6d9-497a-9689-57dc05837dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_data(n=500, p=100, setting='continuous',seed=123):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # 1. Generate Covariates (100 variables)\n",
    "    X = np.random.normal(0, 1, size=(n, p))\n",
    "    \n",
    "    # 2. Define the &quot;True&quot; Confounding Mechanism (The Nuisance Function)\n",
    "    # Only the first 5 variables (indices 0-4) actually matter.\n",
    "    # Interaction: X0 * X1\n",
    "    # Non-linear: X2 squared\n",
    "    # Linear: X3, X4\n",
    "    nuisance_term = 0.5 * X[:,0] + 0.2 * X[:,1] + 0.4 * X[:,2] - 0.3 * X[:,3] - 0.2 * X[:,4] - 0.35 * X[:,5]\n",
    "    # 3. Treatment Assignment (Propensity)\n",
    "    # P(T=1 | X) depends on the nuisance term\n",
    "    logit_p = -0.5 + nuisance_term\n",
    "    prob_t = 1 / (1 + np.exp(-logit_p))\n",
    "    T = np.random.binomial(1, prob_t)\n",
    "    # 4. Outcome Generate\n",
    "    \n",
    "    # Base Log-Odds (without treatment)\n",
    "    logit_y_base = -1.5 + nuisance_term\n",
    "    \n",
    "    # To get the Ground Truth Risk Difference, we simulate the counterfactuals:\n",
    "    # What if everyone was treated?\n",
    "    p1_true = 1 / (1 + np.exp(-(logit_y_base + 0.8 * 1))) \n",
    "    # What if everyone was untreated?\n",
    "    p0_true = 1 / (1 + np.exp(-(logit_y_base + 0.8 * 0)))\n",
    "    \n",
    "    true_ate = np.mean(p1_true - p0_true)\n",
    "    \n",
    "    # Generate Observed Binary Outcome\n",
    "    # We add the treatment effect (0.8 log odds) to the base\n",
    "    logit_y_observed = logit_y_base + 0.8 * T\n",
    "    prob_y_observed = 1 / (1 + np.exp(-logit_y_observed))\n",
    "    Y = np.random.binomial(1, prob_y_observed)\n",
    "    Y = Y.astype(int)\n",
    "    \n",
    "    return X, T, Y, true_ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a02b4c77-f396-4242-bca3-67c49b04a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_linear_simulation(n_epochs=100):\n",
    "    # 1. Initialize a list to store results\n",
    "    results_list = []\n",
    "\n",
    "    print(f\"Starting simulation with {n_epochs} epochs...\")\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        # --- 1. CONTINUOUS OUTCOME ---\n",
    "        X, T, Y, true_eff = generate_linear_data(n=500, p=100, setting='continuous',seed = epoch)\n",
    "        sim = CausalEstimators(X, T, Y, outcome_type='continuous')\n",
    "                \n",
    "        # 3. Apply your 5 different methods\n",
    "        # Replace these with your actual function calls (e.g., DML, PSM, etc.)\n",
    "        \n",
    "        # Method 1: Naive (Unadjusted)\n",
    "        res_m1 = sim.run_naive()\n",
    "        \n",
    "        # Method 2: Regression (Linear)\n",
    "        res_m2 = sim.run_regression_adjustment()\n",
    "        \n",
    "        # Method 3: IPTW (Logistic PS)\n",
    "        res_m3 = sim.run_iptw()\n",
    "        \n",
    "        # Method 4: PSM (1:1 Nearest Neighbor)\n",
    "        res_m4 = sim.run_psm()\n",
    "        \n",
    "        # Method 5: DML (Gradient Boosting)\n",
    "        res_m5 = sim.run_dml()\n",
    "\n",
    "        # 4. Record results for each method in a dictionary\n",
    "        # We store them as separate rows to make it \"tidy\" (Long Format)\n",
    "        methods = {\n",
    "            \"Naive (Unadjusted)\": res_m1,\n",
    "            \"Regression (Logistic):\": res_m2,\n",
    "            \"IPTW (Logistic PS)\": res_m3,\n",
    "            \"PSM (1:1 Nearest Neighbor)\": res_m4,\n",
    "            \"DML (Gradient Boosting)\": res_m5\n",
    "        }\n",
    "\n",
    "        for method_name, value in methods.items():\n",
    "            results_list.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"method\": method_name,\n",
    "                \"estimate\": value,\n",
    "                \"bias\": abs(value - true_eff), # You can add metrics here\n",
    "            })\n",
    "\n",
    "    # 5. Convert the list of dictionaries to a pandas DataFrame\n",
    "    df_results = pd.DataFrame(results_list)\n",
    "    \n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10d5dfed-6e5f-4bf8-90ac-d818d2d0ea78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simulation with 1000 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [23:40<00:00,  1.42s/it]\n"
     ]
    }
   ],
   "source": [
    "# Execute control group\n",
    "df = run_linear_simulation(n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "635c70b8-19e2-451a-8316-9692b2d34bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulation Estimates Summary:\n",
      "                       method      mean       std\n",
      "0     DML (Gradient Boosting)  0.190478  0.043287\n",
      "1          IPTW (Logistic PS)  0.154049  0.052746\n",
      "2          Naive (Unadjusted)  0.255671  0.040736\n",
      "3  PSM (1:1 Nearest Neighbor)  0.162023  0.091481\n",
      "4      Regression (Logistic):  0.128974  0.043140\n"
     ]
    }
   ],
   "source": [
    "# Quick summary of mean estimates across methods\n",
    "summary = df.groupby('method')['estimate'].agg(['mean', 'std']).reset_index()\n",
    "print(\"\\nSimulation Estimates Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04615421-bc47-4a32-99f8-dd6964e1fa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulation Bias Summary:\n",
      "                       method      mean       std\n",
      "0     DML (Gradient Boosting)  0.055210  0.035465\n",
      "1          IPTW (Logistic PS)  0.043147  0.032855\n",
      "2          Naive (Unadjusted)  0.114673  0.040430\n",
      "3  PSM (1:1 Nearest Neighbor)  0.076260  0.054505\n",
      "4      Regression (Logistic):  0.035243  0.027416\n"
     ]
    }
   ],
   "source": [
    "# Quick summary of mean estimates across methods\n",
    "summary = df.groupby('method')['bias'].agg(['mean', 'std']).reset_index()\n",
    "print(\"\\nSimulation Bias Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611367e2-c95f-440c-86b1-fc90d322db8a",
   "metadata": {},
   "source": [
    "## Main group with 100 covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19105085-f270-4b22-9d29-4291b2dab09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(n_epochs=100):\n",
    "    # 1. Initialize a list to store results\n",
    "    results_list = []\n",
    "\n",
    "    print(f\"Starting simulation with {n_epochs} epochs...\")\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        # --- 1. CONTINUOUS OUTCOME ---\n",
    "        X, T, Y, true_eff = generate_bin_data(n=500, p=100, seed = epoch)\n",
    "        sim = CausalEstimators(X, T, Y, outcome_type='continuous')\n",
    "                \n",
    "        # 3. Apply your 5 different methods\n",
    "        # Replace these with your actual function calls (e.g., DML, PSM, etc.)\n",
    "        \n",
    "        # Method 1: Naive (Unadjusted)\n",
    "        res_m1 = sim.run_naive()\n",
    "        \n",
    "        # Method 2: Regression (Linear)\n",
    "        res_m2 = sim.run_regression_adjustment()\n",
    "        \n",
    "        # Method 3: IPTW (Logistic PS)\n",
    "        res_m3 = sim.run_iptw()\n",
    "        \n",
    "        # Method 4: PSM (1:1 Nearest Neighbor)\n",
    "        res_m4 = sim.run_psm()\n",
    "        \n",
    "        # Method 5: DML (Gradient Boosting)\n",
    "        res_m5 = sim.run_dml()\n",
    "\n",
    "        # 4. Record results for each method in a dictionary\n",
    "        # We store them as separate rows to make it \"tidy\" (Long Format)\n",
    "        methods = {\n",
    "            \"Naive (Unadjusted)\": res_m1,\n",
    "            \"Regression (Logistic):\": res_m2,\n",
    "            \"IPTW (Logistic PS)\": res_m3,\n",
    "            \"PSM (1:1 Nearest Neighbor)\": res_m4,\n",
    "            \"DML (Gradient Boosting)\": res_m5\n",
    "        }\n",
    "\n",
    "        for method_name, value in methods.items():\n",
    "            results_list.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"method\": method_name,\n",
    "                \"estimate\": value,\n",
    "                \"bias\": abs(value - true_eff), # You can add metrics here\n",
    "            })\n",
    "\n",
    "    # 5. Convert the list of dictionaries to a pandas DataFrame\n",
    "    df_results = pd.DataFrame(results_list)\n",
    "    \n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbc88c4e-bb51-4fb3-b44d-ee6e34b764c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simulation with 1000 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [23:40<00:00,  1.42s/it]\n"
     ]
    }
   ],
   "source": [
    "# Execute main group\n",
    "df = run_simulation(n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf098067-130e-4d9c-9ce3-346a802cae95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulation Estimates Summary:\n",
      "                       method      mean       std\n",
      "0     DML (Gradient Boosting)  0.229954  0.044883\n",
      "1          IPTW (Logistic PS)  0.251185  0.051680\n",
      "2          Naive (Unadjusted)  0.269620  0.041328\n",
      "3  PSM (1:1 Nearest Neighbor)  0.250257  0.070751\n",
      "4      Regression (Logistic):  0.231013  0.044545\n"
     ]
    }
   ],
   "source": [
    "# Quick summary of mean estimates across methods\n",
    "summary = df.groupby('method')['estimate'].agg(['mean', 'std']).reset_index()\n",
    "print(\"\\nSimulation Estimates Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fab37a04-76a7-46f4-a835-8365dc9fdd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulation Bias Summary:\n",
      "                       method      mean       std\n",
      "0     DML (Gradient Boosting)  0.074249  0.041657\n",
      "1          IPTW (Logistic PS)  0.094572  0.049828\n",
      "2          Naive (Unadjusted)  0.111963  0.041464\n",
      "3  PSM (1:1 Nearest Neighbor)  0.100435  0.059243\n",
      "4      Regression (Logistic):  0.074990  0.041831\n"
     ]
    }
   ],
   "source": [
    "# Quick summary of mean estimates across methods\n",
    "summary = df.groupby('method')['bias'].agg(['mean', 'std']).reset_index()\n",
    "print(\"\\nSimulation Bias Summary:\")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
